['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=0']
Command line arguments: Namespace(channels=[0], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-24 21:04:06.438693
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.715005952
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 202.260870361 168.356483459
20 485.263098145 456.906463623
30 720.617132568 687.411590576
40 970.997924805 928.310668945
50 1267.06549072 1210.00848389
60 1645.51918945 1567.70336914
70 2171.50119629 2065.12841797
80 2995.66479492 2853.98950195
90 4635.590625 4468.23974609
100 101397.328125 96893.890625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.60151[0m     [32m1.18252[0m    1.35433      0.45253  458.75s
      2     [36m0.55444[0m     1.34193    0.41317      0.48267  442.14s
      3     [36m0.51843[0m     1.63951    0.31621      0.48139  386.64s
      4     [36m0.48443[0m     1.30686    0.37068      0.48583  385.81s
      5     [36m0.44287[0m     1.69470    0.26133      0.46636  385.62s
      6     [36m0.37285[0m     1.91227    0.19498      0.44561  385.70s
Early stopping.
Best valid loss was 1.182517 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.451775956284
Error rate (%):  54.8224043716
yVal [0 1 1 ..., 1 1 0]
[[6614  666]
 [7360    0]]
0.389332083508
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=1']
Command line arguments: Namespace(channels=[1], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-24 22:12:20.967047
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714432512
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 206.928393555 172.378547668
20 498.400262451 474.416625977
30 733.650720215 708.896759033
40 974.438952637 945.946472168
50 1245.57879639 1211.81964111
60 1575.56948242 1534.62207031
70 2010.92930908 1959.51019287
80 2664.01162109 2593.76342773
90 3932.5364502 3832.79516602
100 71591.0625 60304.03125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.57469[0m     [32m0.89718[0m    1.75515      0.49547  385.45s
      2     [36m0.61282[0m     0.97909    0.62591      0.49334  386.21s
      3     [36m0.55736[0m     1.22177    0.45619      0.49624  386.13s
      4     [36m0.48272[0m     0.91463    0.52778      0.48386  386.03s
      5     [36m0.38134[0m     [32m0.71314[0m    0.53473      0.58948  386.06s
      6     [36m0.27871[0m     1.29047    0.21598      0.45919  386.42s
      7     [36m0.20118[0m     1.74335    0.11540      0.48318  386.05s
      8     [36m0.12620[0m     1.52765    0.08261      0.50811  386.14s
      9     0.73346     0.95188    0.77053      0.52032  386.28s
     10     0.13596     1.85079    0.07346      0.47310  386.10s
Early stopping.
Best valid loss was 0.713137 at epoch 5.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.59043715847
Error rate (%):  40.956284153
yVal [0 1 1 ..., 1 1 0]
[[4840 2440]
 [3556 3804]]
0.633988695578
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=2']
Command line arguments: Namespace(channels=[2], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-24 23:44:32.910325
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714715136
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 213.738221741 179.564910889
20 508.706988525 488.186553955
30 740.875622559 721.752166748
40 972.295471191 952.076049805
50 1226.75701904 1203.95013428
60 1529.04313965 1501.61279297
70 1918.31774902 1884.3873291
80 2484.309375 2442.01074219
90 3547.98891602 3499.03417969
100 119907.929688 146484.890625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.47210[0m     [32m0.83626[0m    1.76035      0.49027  386.21s
      2     [36m0.58579[0m     [32m0.79152[0m    0.74008      0.49488  386.52s
      3     [36m0.54918[0m     0.89203    0.61565      0.40147  386.76s
      4     [36m0.51434[0m     1.00243    0.51309      0.45048  387.13s
      5     [36m0.45550[0m     1.13225    0.40229      0.43024  486.59s
      6     [36m0.40576[0m     0.91490    0.44351      0.39882  425.86s
      7     [36m0.35794[0m     1.45119    0.24666      0.42529  386.94s
Early stopping.
Best valid loss was 0.791519 at epoch 2.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.484904371585
Error rate (%):  51.5095628415
yVal [0 1 1 ..., 1 1 0]
[[4485 2795]
 [4746 2614]]
0.500943238996
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=3']
Command line arguments: Namespace(channels=[3], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 00:43:21.689733
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.713740288
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 19.6690460205 14.0969910622
20 116.733595276 99.3462142944
30 468.487338257 443.317764282
40 731.003540039 706.207275391
50 987.08303833 959.834991455
60 1271.64260254 1238.17504883
70 1621.35058594 1579.47271729
80 2106.56230469 2052.5390625
90 2961.25544434 2887.56567383
100 141686.875 89376.015625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.22510[0m     [32m1.33695[0m    0.91633      0.42896  383.75s
      2     [36m0.55191[0m     [32m1.20029[0m    0.45982      0.39916  385.08s
      3     [36m0.45655[0m     [32m1.14375[0m    0.39917      0.41513  385.05s
      4     [36m0.40270[0m     1.52619    0.26386      0.40548  385.14s
      5     [36m0.36086[0m     1.88270    0.19167      0.38960  385.42s
      6     [36m0.32461[0m     2.25874    0.14371      0.38055  385.02s
      7     0.47731     1.70004    0.28076      0.42666  385.08s
      8     [36m0.28183[0m     2.23384    0.12617      0.40847  385.13s
Early stopping.
Best valid loss was 1.143748 at epoch 3.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.414412568306
Error rate (%):  58.5587431694
yVal [0 1 1 ..., 1 1 0]
[[5009 2271]
 [6302 1058]]
0.470392183394
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=4']
Command line arguments: Namespace(channels=[4], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 02:04:28.528351
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714100736
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 239.421202087 199.921806335
20 574.585070801 543.65411377
30 843.210821533 809.075256348
40 1119.16928711 1078.46875
50 1437.58898926 1387.05267334
60 1842.8963623 1777.23901367
70 2410.89768066 2320.98596191
80 3299.05410156 3167.82080078
90 4954.51757812 4739.92138672
100 52009.53125 42999.078125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.56327[0m     [32m0.76466[0m    2.04439      0.46004  384.37s
      2     [36m0.63415[0m     0.93570    0.67773      0.49454  384.95s
      3     [36m0.60892[0m     1.25863    0.48380      0.49744  384.88s
      4     [36m0.55377[0m     1.69312    0.32707      0.49684  385.02s
      5     [36m0.45731[0m     2.48141    0.18429      0.49718  384.88s
      6     0.69935     1.55972    0.44838      0.49710  384.74s
Early stopping.
Best valid loss was 0.764661 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.453756830601
Error rate (%):  54.6243169399
yVal [0 1 1 ..., 1 1 0]
[[6643  637]
 [7360    0]]
0.393122331134
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=5']
Command line arguments: Namespace(channels=[5], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 02:54:31.419879
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714244096
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 235.012228394 192.875953674
20 566.306152344 529.812561035
30 831.834051514 789.432739258
40 1104.28349609 1051.81738281
50 1420.10662842 1351.90142822
60 1824.84020996 1734.17260742
70 2397.64912109 2271.95996094
80 3280.87207031 3104.30908203
90 4885.48979492 4615.77099609
100 55591.203125 52662.2460938
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.61485[0m     [32m0.66467[0m    2.42955      0.65275  384.37s
      2     [36m0.63451[0m     0.71952    0.88184      0.47336  385.06s
      3     [36m0.57810[0m     0.74108    0.78009      0.51452  384.94s
      4     [36m0.51103[0m     0.75089    0.68056      0.50990  384.86s
      5     [36m0.43280[0m     0.89271    0.48481      0.50435  384.79s
      6     [36m0.32359[0m     1.86863    0.17317      0.48190  384.90s
Early stopping.
Best valid loss was 0.664672 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.65662568306
Error rate (%):  34.337431694
yVal [0 1 1 ..., 1 1 0]
[[2353 4927]
 [ 100 7260]]
0.644857364952
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=6']
Command line arguments: Namespace(channels=[6], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 03:44:27.125322
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714637312
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 202.214730835 163.390800476
20 485.357543945 447.911590576
30 715.467364502 669.537719727
40 956.924304199 896.82824707
50 1245.06646729 1163.88885498
60 1624.86760254 1511.64050293
70 2166.63815918 2006.6552124
80 3004.99970703 2778.06762695
90 4614.8956543 4242.0456543
100 66751.359375 59361.5703125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.52768[0m     [32m0.66785[0m    2.28744      0.49573  384.50s
      2     [36m0.61704[0m     0.77388    0.79733      0.48463  384.93s
      3     [36m0.56629[0m     0.82716    0.68462      0.45159  384.75s
      4     [36m0.49248[0m     0.96501    0.51034      0.46687  384.93s
      5     [36m0.39118[0m     1.16817    0.33487      0.46440  384.87s
      6     [36m0.30487[0m     1.16237    0.26228      0.49778  385.01s
Early stopping.
Best valid loss was 0.667854 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.497336065574
Error rate (%):  50.2663934426
yVal [0 1 1 ..., 1 1 0]
[[3293 3987]
 [3372 3988]]
0.507107863264
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=7']
Command line arguments: Namespace(channels=[7], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 04:35:14.364498
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714706944
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 185.573376465 149.823059082
20 446.898638916 411.701080322
30 662.398498535 618.625640869
40 892.103625488 835.423828125
50 1170.04150391 1093.59197998
60 1538.95534668 1432.44165039
70 2070.14841309 1921.18255615
80 2918.14931641 2705.77734375
90 4599.92104492 4269.7043457
100 65353.1796875 57503.8945312
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.50526[0m     [32m0.73383[0m    2.05123      0.49394  385.98s
      2     [36m0.61139[0m     0.73580    0.83091      0.47874  386.58s
      3     [36m0.55802[0m     0.74324    0.75079      0.58367  386.11s
      4     [36m0.49950[0m     [32m0.60258[0m    0.82893      0.56335  386.00s
      5     [36m0.42773[0m     0.70285    0.60856      0.58478  386.50s
      6     0.46062     [32m0.58485[0m    0.78759      0.62133  386.36s
      7     [36m0.25593[0m     0.75057    0.34099      0.66470  386.28s
      8     [36m0.18340[0m     0.84402    0.21729      0.57386  386.35s
      9     [36m0.11757[0m     1.15934    0.10141      0.56463  386.40s
     10     [36m0.07523[0m     1.42952    0.05263      0.58683  386.31s
     11     [36m0.07352[0m     1.59484    0.04610      0.56276  386.10s
Early stopping.
Best valid loss was 0.584848 at epoch 6.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.621584699454
Error rate (%):  37.8415300546
yVal [0 1 1 ..., 1 1 0]
[[5123 2157]
 [3383 3977]]
0.740370879121
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=8']
Command line arguments: Namespace(channels=[8], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 05:58:13.602484
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714399744
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 182.433731079 149.94278717
20 436.049041748 410.029296875
30 635.802856445 606.535400391
40 835.57677002 801.683410645
50 1056.74621582 1016.02960205
60 1321.59230957 1272.10205078
70 1665.66821289 1603.8125
80 2168.0465332 2087.17016602
90 3108.63288574 2996.27380371
100 59862.1054688 57871.4179688
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.25648[0m     [32m0.81809[0m    1.53587      0.48685  385.18s
      2     [36m0.61682[0m     1.36938    0.45044      0.49949  385.56s
      3     [36m0.56077[0m     1.61210    0.34785      0.49898  385.09s
      4     [36m0.50768[0m     2.16627    0.23436      0.49889  385.22s
      5     [36m0.44734[0m     2.17441    0.20573      0.49863  385.31s
      6     [36m0.32008[0m     3.44502    0.09291      0.49906  385.26s
Early stopping.
Best valid loss was 0.818093 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.484699453552
Error rate (%):  51.5300546448
yVal [0 1 1 ..., 1 1 0]
[[7096  184]
 [7360    0]]
0.499477434081
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=9']
Command line arguments: Namespace(channels=[9], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 06:48:01.478741
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714706944
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 3.73858091831 3.27668011189
20 8.95421485901 8.65685558319
30 13.5715697289 13.20033741
40 19.6790660858 19.0048999786
50 43.6948184967 34.7788333893
60 449.341595459 388.074157715
70 780.715905762 715.159332275
80 1201.33637695 1117.43395996
90 1923.547229 1805.91333008
100 43853.8789062 40357.3945312
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.92593[0m     [32m0.75332[0m    1.22914      0.44988  384.68s
      2     [36m0.66352[0m     1.05297    0.63014      0.45321  385.05s
      3     [36m0.62574[0m     0.94048    0.66534      0.44459  384.94s
      4     [36m0.56216[0m     1.12439    0.49997      0.44023  384.82s
      5     [36m0.49057[0m     1.61905    0.30300      0.51221  384.92s
      6     [36m0.39618[0m     2.19270    0.18068      0.50811  384.76s
Early stopping.
Best valid loss was 0.753316 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.443784153005
Error rate (%):  55.6215846995
yVal [0 1 1 ..., 1 1 0]
[[3530 3750]
 [4393 2967]]
0.405849380748
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=10']
Command line arguments: Namespace(channels=[10], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 07:37:40.823034
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714657792
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 184.185713196 152.952148438
20 440.024975586 416.685943604
30 641.064581299 616.230438232
40 840.985620117 812.821044922
50 1060.82244873 1027.43164062
60 1320.46096191 1279.14282227
70 1649.28624268 1598.09313965
80 2109.55322266 2045.45068359
90 2906.99501953 2825.17199707
100 41643.4882812 36225.46875
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.25655[0m     [32m0.73964[0m    1.69888      0.41487  384.83s
      2     [36m0.58134[0m     1.56839    0.37066      0.49940  385.30s
      3     [36m0.52559[0m     1.23765    0.42466      0.49454  384.92s
      4     [36m0.50102[0m     1.33869    0.37426      0.49121  385.02s
      5     [36m0.39429[0m     1.70738    0.23093      0.49232  384.98s
      6     [36m0.28327[0m     2.49042    0.11374      0.49069  385.10s
Early stopping.
Best valid loss was 0.739635 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.42206284153
Error rate (%):  57.793715847
yVal [0 1 1 ..., 1 1 0]
[[5751 1529]
 [6932  428]]
0.454614078177
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=11']
Command line arguments: Namespace(channels=[11], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 08:28:02.589222
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714559488
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 175.594898987 145.517463684
20 419.360888672 395.677886963
30 609.753845215 583.460235596
40 797.140246582 765.498535156
50 1000.32189941 962.170196533
60 1235.86208496 1189.52197266
70 1531.12805176 1473.54846191
80 1943.29196777 1868.76367188
90 2665.28051758 2562.52746582
100 34311.5273438 30092.4199219
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.17491[0m     [32m0.82281[0m    1.42792      0.37082  384.39s
      2     [36m0.50531[0m     1.58090    0.31963      0.47695  384.94s
      3     [36m0.44032[0m     1.59607    0.27588      0.44672  384.73s
      4     [36m0.37933[0m     4.40821    0.08605      0.47720  384.61s
      5     [36m0.36142[0m     1.78945    0.20197      0.40958  385.00s
      6     [36m0.23405[0m     2.64387    0.08853      0.44775  385.00s
Early stopping.
Best valid loss was 0.822814 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.376980874317
Error rate (%):  62.3019125683
yVal [0 1 1 ..., 1 1 0]
[[5519 1761]
 [7360    0]]
0.410507989802
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=12']
Command line arguments: Namespace(channels=[12], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 09:19:00.108633
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714534912
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 197.847105408 162.232131958
20 466.161206055 435.430114746
30 668.632995605 633.256835938
40 862.47142334 820.440124512
50 1068.59735107 1017.74472046
60 1307.13903809 1244.34411621
70 1610.01538086 1529.31481934
80 2051.1934082 1942.77392578
90 2874.82258301 2724.64428711
100 68176.640625 59867.6914062
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.22364[0m     [32m0.65874[0m    1.85755      0.60681  386.03s
      2     [36m0.57716[0m     0.67470    0.85544      0.63823  386.79s
      3     [36m0.53005[0m     0.81621    0.64941      0.60494  386.89s
      4     [36m0.47938[0m     0.84883    0.56475      0.62047  387.22s
      5     [36m0.40124[0m     0.95229    0.42134      0.57684  386.78s
      6     0.40942     1.13759    0.35990      0.55661  386.64s
Early stopping.
Best valid loss was 0.658741 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.613797814208
Error rate (%):  38.6202185792
yVal [0 1 1 ..., 1 1 0]
[[2237 5043]
 [ 611 6749]]
0.575741739578
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=13']
Command line arguments: Namespace(channels=[13], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 10:08:49.948109
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714461184
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 200.816160583 166.028656006
20 476.097589111 449.419219971
30 685.35871582 656.065368652
40 886.69173584 852.978759766
50 1100.39276123 1060.0045166
60 1345.78920898 1295.31213379
70 1653.55772705 1589.07421875
80 2092.56630859 2005.52539062
90 2905.67011719 2776.49841309
100 37327.25 36168.203125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.26479[0m     [32m0.69167[0m    1.82859      0.58530  386.89s
      2     [36m0.60045[0m     0.73389    0.81817      0.49957  387.02s
      3     [36m0.54307[0m     1.05807    0.51327      0.52502  386.70s
      4     [36m0.46555[0m     1.48224    0.31409      0.52596  386.41s
      5     0.49669     1.01092    0.49133      0.50410  386.11s
      6     [36m0.33964[0m     2.60145    0.13056      0.52604  386.16s
Early stopping.
Best valid loss was 0.691675 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.591120218579
Error rate (%):  40.8879781421
yVal [0 1 1 ..., 1 1 0]
[[1294 5986]
 [   0 7360]]
0.564475959672
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=14']
Command line arguments: Namespace(channels=[14], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 10:58:52.527009
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.715198464
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 192.278979492 159.171096802
20 454.51661377 429.81652832
30 651.260583496 624.928649902
40 837.994689941 807.764465332
50 1032.79595947 997.46282959
60 1252.28759766 1210.24035645
70 1520.01416016 1466.78344727
80 1885.53779297 1816.83203125
90 2515.98427734 2416.08642578
100 43412.1367188 33402.015625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.19086[0m     [32m0.80562[0m    1.47818      0.54192  386.19s
      2     [36m0.62374[0m     [32m0.61036[0m    1.02192      0.66402  386.28s
      3     [36m0.55895[0m     0.62896    0.88870      0.55063  386.60s
      4     [36m0.54047[0m     0.63261    0.85435      0.51767  386.65s
      5     [36m0.44006[0m     0.71936    0.61174      0.56796  386.65s
      6     0.56850     0.67427    0.84314      0.65557  386.81s
      7     [36m0.35146[0m     0.72659    0.48371      0.64583  386.97s
Early stopping.
Best valid loss was 0.610358 at epoch 2.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.664754098361
Error rate (%):  33.5245901639
yVal [0 1 1 ..., 1 1 0]
[[2656 4624]
 [ 284 7076]]
0.679533834881
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run1.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu0', '--data-path=/local/seizure_detection/data', '--channels=15']
Command line arguments: Namespace(channels=[15], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/local/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run1.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu0')
Git reference:  Timestamp: 2016-09-25 11:56:08.848790
Hostname: paladin
gpu0
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 512
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net0
evaluation:
    online_training: False
    model: net0

end Configuration
Loading and preprocessing data...
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/local/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.714326016
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 195.569773865 161.052391052
20 461.383886719 434.366851807
30 660.075195312 630.937469482
40 847.686938477 814.774291992
50 1042.60760498 1005.25018311
60 1261.60791016 1217.76916504
70 1528.51779785 1475.94433594
80 1895.11066895 1831.36975098
90 2537.23972168 2455.97021484
100 29197.0292969 29615.0136719
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net0
Model name for the evaluation phase:  net0
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x512x64
  1          64x512x64
  2          64x512x64
  3          64x256x32
  4          64x256x32
  5          64x256x32
  6          64x256x32
  7          64x128x16
  8          64x128x16
  9          64x128x16
 10          2x128x16
 11          2
 12          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.18106[0m     [32m0.66974[0m    1.76346      0.54244  387.38s
      2     [36m0.61613[0m     0.69555    0.88582      0.55550  386.59s
      3     [36m0.56325[0m     0.75397    0.74704      0.55439  386.04s
      4     [36m0.50957[0m     0.79781    0.63871      0.54722  386.28s
      5     [36m0.44576[0m     0.86801    0.51355      0.59153  386.59s
      6     [36m0.33921[0m     0.84215    0.40279      0.65736  386.20s
Early stopping.
Best valid loss was 0.669742 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d6' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d6' (shape 64).
Loaded parameters to layer 'conv2d9' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d9' (shape 64).
Loaded parameters to layer 'conv2d10' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d10' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.54924863388
Error rate (%):  45.075136612
yVal [0 1 1 ..., 1 1 0]
[[3898 3382]
 [3217 4143]]
0.57033115407
