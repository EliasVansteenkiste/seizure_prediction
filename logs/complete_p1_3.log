['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=0']
Command line arguments: Namespace(channels=[0], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 11:54:08.960774
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.788197376
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 202.260870361 168.356483459
20 485.263098145 456.906463623
30 720.617132568 687.411590576
40 970.997924805 928.310668945
50 1267.06549072 1210.00848389
60 1645.51918945 1567.70336914
70 2171.50119629 2065.12841797
80 2995.66479492 2853.98950195
90 4635.590625 4468.23974609
100 101397.328125 96893.890625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.04989[0m     [32m0.88114[0m    1.19151      0.36800  120.70s
      2     [36m0.58927[0m     0.94830    0.62140      0.35912  124.42s
      3     [36m0.56906[0m     1.06710    0.53327      0.36817  124.94s
      4     [36m0.53838[0m     0.94096    0.57216      0.44126  125.99s
      5     [36m0.50291[0m     1.02652    0.48992      0.41983  125.52s
Early stopping.
Best valid loss was 0.881145 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.37356557377
Error rate (%):  62.643442623
yVal [0 1 1 ..., 1 1 0]
[[5131 2149]
 [7022  338]]
roc_auc: 0.400199950355
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.10188707  0.89811265]
[ 0.56246603  0.4375346 ]
[ 0.98211473  0.01788617]
[ 0.65968984  0.3403098 ]
[ 0.66887617  0.33112365]
[ 0.55478424  0.44521636]
[  9.99984503e-01   1.72611071e-05]
[ 0.76762265  0.23237754]
[ 0.46622992  0.53377032]
[ 0.98850036  0.01150103]
[ 0.98943919  0.01056251]
[  9.99861956e-01   1.38455929e-04]
[  9.99986768e-01   1.38172018e-05]
[ 0.93659037  0.06340907]
[ 0.77351755  0.22648281]
[ 0.99607396  0.00392788]
[ 0.13252297  0.86747783]
[ 0.50744736  0.49255216]
[ 0.78895307  0.21104653]
[ 0.93140471  0.06859794]
[ 0.99060875  0.00939053]
[ 0.86707276  0.13292734]
[ 0.82417601  0.17582375]
[  9.99963045e-01   3.85328967e-05]
[ 0.6935991   0.30640191]
[  9.99641240e-01   3.59016296e-04]
[  9.99943614e-01   5.74802471e-05]
[  9.99999285e-01   1.04834601e-06]
[ 0.31780738  0.6821931 ]
[ 0.87397379  0.12602624]
[ 0.97275007  0.02725077]
[ 0.34140614  0.65859491]
[ 0.98691344  0.01308723]
[  9.99671340e-01   3.30067414e-04]
[  1.00000000e+00   4.93218479e-08]
[ 0.99003738  0.00996284]
[ 0.80959505  0.19039966]
[ 0.96992189  0.03007807]
[ 0.84062064  0.15937987]
[  9.99996066e-01   3.96734868e-06]
[ 0.89447296  0.10553164]
[ 0.62523407  0.37476593]
[ 0.98005378  0.01994817]
[  9.99998093e-01   2.93863627e-06]
[ 0.84387076  0.15612982]
[ 0.81136572  0.18863505]
[ 0.99738419  0.00261598]
[ 0.57994246  0.4200573 ]
[ 0.9980095   0.00199053]
[ 0.51865876  0.48134127]
[ 0.51169807  0.48830193]
[ 0.74948126  0.25051811]
[ 0.9600116  0.0399894]
[ 0.0208945   0.97910535]
[ 0.94474608  0.05525462]
[ 0.92417264  0.07582846]
[ 0.49485984  0.50514024]
[ 0.21688913  0.78311062]
[ 0.55818611  0.44181386]
[ 0.23751223  0.76248837]
[ 0.20898862  0.79101133]
[  1.00000000e+00   2.57704528e-07]
[ 0.79655206  0.20344459]
[ 0.23806114  0.76193833]
[ 0.26589945  0.73410088]
[ 0.13925502  0.86074477]
[ 0.40785906  0.59214139]
[ 0.63357669  0.36642385]
[ 0.37955895  0.62044132]
[ 0.74567598  0.25432387]
[ 0.88497156  0.11502824]
[ 0.84421277  0.15578692]
[ 0.99898857  0.00101189]
[ 0.99082917  0.00917359]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.92183602  0.07816432]
[ 0.89947051  0.10052957]
[ 0.99021196  0.0097889 ]
[ 0.99711663  0.00288532]
[ 0.99752462  0.00247895]
roc_auc for the hours: 0.294594594595
Accuracy validation for the hours:  0.746835443038
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=1']
Command line arguments: Namespace(channels=[1], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 12:11:35.632013
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.788983808
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 206.928393555 172.378547668
20 498.400262451 474.416625977
30 733.650720215 708.896759033
40 974.438952637 945.946472168
50 1245.57879639 1211.81964111
60 1575.56948242 1534.62207031
70 2010.92930908 1959.51019287
80 2664.01162109 2593.76342773
90 3932.5364502 3832.79516602
100 71591.0625 60304.03125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.04727[0m     [32m0.69165[0m    1.51416      0.43870  122.80s
      2     [36m0.63372[0m     0.69437    0.91266      0.48916  122.98s
      3     [36m0.60814[0m     0.73551    0.82684      0.50068  124.71s
      4     [36m0.58764[0m     0.69393    0.84682      0.51631  124.92s
      5     [36m0.56493[0m     0.69399    0.81402      0.54167  126.05s
Early stopping.
Best valid loss was 0.691650 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.444330601093
Error rate (%):  55.5669398907
yVal [0 1 1 ..., 1 1 0]
[[4606 2674]
 [5461 1899]]
roc_auc: 0.487745031802
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.5412876   0.45871267]
[ 0.46089882  0.53910124]
[ 0.69073611  0.30926391]
[ 0.0073796   0.99262053]
[ 0.07289525  0.92710531]
[ 0.24697854  0.753021  ]
[ 0.92064011  0.07936013]
[ 0.44859403  0.55140638]
[ 0.0798985  0.920102 ]
[ 0.75440449  0.24559495]
[ 0.72006178  0.27993816]
[ 0.95391983  0.04608094]
[ 0.79840422  0.20159598]
[ 0.36542824  0.63457268]
[ 0.66722602  0.3327738 ]
[ 0.34874123  0.65125835]
[ 0.34654027  0.65345991]
[ 0.1602302   0.83976883]
[ 0.65769118  0.34230855]
[ 0.91945088  0.08055   ]
[ 0.95808238  0.0419186 ]
[ 0.65607405  0.34392631]
[ 0.81632102  0.18367916]
[ 0.92362601  0.07637449]
[ 0.27864563  0.72135502]
[  9.99127448e-01   8.74022022e-04]
[ 0.99637103  0.00363046]
[ 0.94472855  0.05527087]
[ 0.18113178  0.81886864]
[ 0.75738037  0.24261965]
[ 0.45620906  0.54379106]
[ 0.31622979  0.68376976]
[ 0.99342406  0.00657664]
[ 0.76308614  0.23691323]
[ 0.88560373  0.11439642]
[ 0.81848997  0.18151014]
[ 0.79970276  0.20029649]
[ 0.6781525   0.32184708]
[ 0.70783955  0.29216036]
[  9.99983251e-01   1.68216193e-05]
[ 0.79753768  0.2024627 ]
[ 0.10916112  0.89083707]
[ 0.72781599  0.2721839 ]
[ 0.68872309  0.3112781 ]
[ 0.82023758  0.17976323]
[ 0.7042712   0.29572836]
[ 0.45788142  0.54211843]
[ 0.86705154  0.13294907]
[ 0.95720607  0.04279413]
[ 0.84079009  0.15921012]
[ 0.28289947  0.71709996]
[ 0.51473361  0.48526636]
[ 0.27515742  0.72484249]
[ 0.82370538  0.17629433]
[ 0.96369278  0.03630773]
[ 0.92412722  0.07587264]
[ 0.13018198  0.8698169 ]
[ 0.54878682  0.45121253]
[ 0.60715467  0.39284489]
[ 0.34109032  0.65890992]
[ 0.79898524  0.20101406]
[  9.99981284e-01   1.88669965e-05]
[ 0.82288349  0.17711577]
[ 0.98262411  0.01737632]
[ 0.73284477  0.26715595]
[ 0.49122486  0.50877517]
[ 0.44859442  0.55140561]
[ 0.5967527   0.40324768]
[ 0.2675997   0.73240054]
[ 0.88804668  0.11195313]
[ 0.96662647  0.03337436]
[ 0.88430154  0.11569839]
[ 0.79519367  0.20480599]
[ 0.98714918  0.0128508 ]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.36471137  0.63528818]
[ 0.89585823  0.10414144]
[ 0.68072319  0.31927702]
[ 0.88898444  0.1110163 ]
[ 0.80729163  0.19270895]
roc_auc for the hours: 0.435135135135
Accuracy validation for the hours:  0.658227848101
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=2']
Command line arguments: Namespace(channels=[2], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 12:29:11.923722
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.789377024
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 213.738221741 179.564910889
20 508.706988525 488.186553955
30 740.875622559 721.752166748
40 972.295471191 952.076049805
50 1226.75701904 1203.95013428
60 1529.04313965 1501.61279297
70 1918.31774902 1884.3873291
80 2484.309375 2442.01074219
90 3547.98891602 3499.03417969
100 119907.929688 146484.890625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.02719[0m     [32m0.68273[0m    1.50455      0.52937  123.71s
      2     [36m0.63252[0m     0.70159    0.90155      0.49607  124.69s
      3     [36m0.59972[0m     0.74546    0.80450      0.44740  124.45s
      4     [36m0.57388[0m     0.72972    0.78643      0.48198  125.25s
      5     [36m0.53543[0m     0.81803    0.65453      0.46866  126.06s
Early stopping.
Best valid loss was 0.682727 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.524043715847
Error rate (%):  47.5956284153
yVal [0 1 1 ..., 1 1 0]
[[2575 4705]
 [2263 5097]]
roc_auc: 0.502731986085
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.07603987  0.92395997]
[ 0.22979201  0.77020788]
[ 0.6039412   0.39605978]
[ 0.16084535  0.83915514]
[ 0.14428604  0.85571432]
[ 0.13600843  0.8639918 ]
[ 0.95571518  0.04428492]
[ 0.1463325   0.85366744]
[ 0.12023439  0.87976587]
[ 0.33541998  0.66457987]
[ 0.2823433   0.71765727]
[ 0.30733311  0.69266707]
[ 0.26748833  0.7325117 ]
[ 0.02036251  0.97963715]
[ 0.17947398  0.820526  ]
[ 0.19466448  0.80533534]
[ 0.07787306  0.92212677]
[  2.71044293e-04   9.99730647e-01]
[ 0.02698482  0.9730143 ]
[ 0.72160953  0.27838978]
[ 0.49471682  0.50528336]
[ 0.23713723  0.76286292]
[ 0.05261196  0.94738787]
[ 0.42788252  0.57211721]
[ 0.00990273  0.99009651]
[ 0.27388093  0.7261197 ]
[ 0.45349783  0.54650241]
[ 0.76598734  0.23401347]
[ 0.15084431  0.84915543]
[ 0.31172463  0.6882751 ]
[ 0.65232277  0.34767666]
[ 0.15846205  0.84153813]
[ 0.99100226  0.00899796]
[ 0.40968665  0.59031308]
[ 0.55414724  0.44585288]
[ 0.33072519  0.66927463]
[ 0.78951818  0.21048738]
[ 0.46885669  0.53114343]
[ 0.29629272  0.7037071 ]
[  9.99989629e-01   1.03971634e-05]
[ 0.77938771  0.22060926]
[ 0.35425791  0.64574051]
[ 0.22541931  0.77458054]
[ 0.57906878  0.42093095]
[ 0.49916479  0.50083566]
[ 0.69274521  0.30725497]
[ 0.31691605  0.68308365]
[ 0.47438112  0.52561831]
[ 0.63945377  0.36054671]
[ 0.63765377  0.36234576]
[ 0.35426188  0.64573878]
[ 0.57225585  0.42774397]
[ 0.64524615  0.35475439]
[ 0.1253397   0.87466019]
[ 0.98271567  0.01728428]
[ 0.9824006   0.01760012]
[ 0.48049664  0.51950347]
[ 0.18972391  0.81027699]
[ 0.4957535   0.50424641]
[ 0.39574203  0.60425818]
[ 0.69706601  0.30293411]
[ 0.9172793   0.08272165]
[ 0.69868731  0.30131164]
[ 0.22214529  0.77785462]
[ 0.36926639  0.63073349]
[ 0.5584231   0.44157696]
[ 0.42377654  0.57622349]
[ 0.33143613  0.66856331]
[ 0.16740283  0.83259684]
[ 0.28533164  0.71466839]
[ 0.43366334  0.56633687]
[ 0.24887851  0.75112128]
[ 0.74537277  0.25462744]
[ 0.98667234  0.01332764]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.47452593  0.52547389]
[ 0.18049195  0.81950736]
[ 0.33542812  0.66457164]
[ 0.50019079  0.49980927]
[ 0.50130951  0.49869049]
roc_auc for the hours: 0.464864864865
Accuracy validation for the hours:  0.341772151899
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=3']
Command line arguments: Namespace(channels=[3], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 12:46:42.830868
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.78956544
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 19.6690460205 14.0969910622
20 116.733595276 99.3462142944
30 468.487338257 443.317764282
40 731.003540039 706.207275391
50 987.08303833 959.834991455
60 1271.64260254 1238.17504883
70 1621.35058594 1579.47271729
80 2106.56230469 2052.5390625
90 2961.25544434 2887.56567383
100 141686.875 89376.015625
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.94737[0m     [32m0.96592[0m    0.98080      0.40634  123.68s
      2     [36m0.59591[0m     [32m0.95262[0m    0.62555      0.44971  124.70s
      3     [36m0.56254[0m     [32m0.94498[0m    0.59529      0.44886  125.71s
      4     [36m0.54194[0m     [32m0.88003[0m    0.61582      0.44237  124.94s
      5     [36m0.52623[0m     [32m0.86715[0m    0.60684      0.52613  124.03s
      6     [36m0.51070[0m     1.06836    0.47802      0.53134  124.41s
      7     [36m0.47788[0m     1.15037    0.41541      0.46218  125.57s
      8     [36m0.41761[0m     1.32902    0.31423      0.42905  126.37s
      9     [36m0.37604[0m     1.25649    0.29928      0.43938  125.28s
Early stopping.
Best valid loss was 0.867152 at epoch 5.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.526297814208
Error rate (%):  47.3702185792
yVal [0 1 1 ..., 1 1 0]
[[4130 3150]
 [3785 3575]]
roc_auc: 0.512335360801
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.70093703  0.29906312]
[ 0.75029778  0.24970241]
[ 0.94891846  0.05108317]
[ 0.23500367  0.76499659]
[ 0.48203039  0.51796991]
[ 0.72643018  0.27356955]
[  9.99991179e-01   9.45666670e-06]
[ 0.72458357  0.27541584]
[ 0.1961136   0.80388647]
[ 0.93702066  0.06297898]
[ 0.52342004  0.47658032]
[ 0.75283742  0.24716249]
[ 0.80098367  0.19901802]
[ 0.24019371  0.75980604]
[ 0.86563766  0.13436329]
[ 0.78067219  0.21932805]
[ 0.43557069  0.56442946]
[ 0.46316746  0.53683263]
[ 0.71370858  0.28629133]
[ 0.90568244  0.09431844]
[ 0.83633888  0.16366206]
[ 0.45061532  0.54938412]
[ 0.66075575  0.33924419]
[ 0.99894869  0.00105136]
[ 0.05407275  0.9459275 ]
[ 0.9650479   0.03495239]
[  9.99921203e-01   7.88034376e-05]
[  9.99991477e-01   8.82382847e-06]
[ 0.13511489  0.86488491]
[ 0.42774773  0.57225269]
[ 0.56682372  0.43317634]
[ 0.43161073  0.56838894]
[ 0.86757821  0.13242114]
[ 0.99895072  0.00105249]
[ 0.99561423  0.00438637]
[  9.99752164e-01   2.51024088e-04]
[ 0.77640212  0.22359428]
[  9.99998808e-01   2.21643018e-06]
[ 0.85299641  0.14700443]
[ 0.54732692  0.45267284]
[ 0.87821913  0.12178531]
[ 0.74798912  0.25201207]
[ 0.98749882  0.01250216]
[ 0.21621184  0.78378808]
[ 0.96827888  0.03172158]
[ 0.77086174  0.22913814]
[  9.99490142e-01   5.12022583e-04]
[ 0.48758057  0.51241952]
[ 0.45020851  0.54979181]
[ 0.46501711  0.5349831 ]
[ 0.8431024   0.15689749]
[ 0.27791888  0.72208124]
[ 0.68580025  0.31419998]
[ 0.3797555   0.62024462]
[ 0.82160103  0.17839843]
[ 0.90992534  0.09007485]
[ 0.69131708  0.3086836 ]
[ 0.8645786  0.1354222]
[  9.99484003e-01   5.18002897e-04]
[ 0.93188143  0.06811854]
[ 0.99898076  0.00102127]
[  1.00000000e+00   4.01892242e-10]
[ 0.73384172  0.2661556 ]
[ 0.65602201  0.34397802]
[ 0.92437828  0.07562214]
[ 0.83948445  0.16051573]
[ 0.92411023  0.07589024]
[ 0.99751884  0.00248209]
[  9.99962211e-01   3.90417445e-05]
[ 0.53240013  0.46759942]
[ 0.35027945  0.64972138]
[ 0.31659663  0.68340313]
[  9.99995708e-01   5.18219258e-06]
[ 0.89940524  0.10059499]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.47265393  0.52734596]
[ 0.78013468  0.21986531]
[ 0.3878769   0.61212319]
[ 0.97658402  0.02341724]
[  9.99991953e-01   8.42175359e-06]
roc_auc for the hours: 0.478378378378
Accuracy validation for the hours:  0.721518987342
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=4']
Command line arguments: Namespace(channels=[4], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 13:12:40.080646
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.78903296
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 239.421202087 199.921806335
20 574.585070801 543.65411377
30 843.210821533 809.075256348
40 1119.16928711 1078.46875
50 1437.58898926 1387.05267334
60 1842.8963623 1777.23901367
70 2410.89768066 2320.98596191
80 3299.05410156 3167.82080078
90 4954.51757812 4739.92138672
100 52009.53125 42999.078125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.10929[0m     [32m0.83661[0m    1.32594      0.50196  123.15s
      2     [36m0.67113[0m     [32m0.74074[0m    0.90604      0.45065  125.15s
      3     [36m0.64943[0m     [32m0.69764[0m    0.93089      0.46482  123.03s
      4     [36m0.63299[0m     0.71859    0.88087      0.46209  123.14s
      5     [36m0.60710[0m     0.76577    0.79279      0.48019  124.76s
      6     [36m0.54369[0m     0.95411    0.56984      0.49377  126.66s
      7     [36m0.45987[0m     1.01675    0.45229      0.53979  125.55s
Early stopping.
Best valid loss was 0.697640 at epoch 3.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.471174863388
Error rate (%):  52.8825136612
yVal [0 1 1 ..., 1 1 0]
[[1797 5483]
 [2259 5101]]
roc_auc: 0.44961562724
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.63000995  0.3699899 ]
[ 0.28066096  0.71933913]
[ 0.37332043  0.62667966]
[ 0.12333369  0.87666547]
[ 0.30602348  0.69397712]
[ 0.80197555  0.19802545]
[  9.99998391e-01   2.40364284e-06]
[ 0.48927093  0.51072943]
[ 0.19493075  0.80506992]
[ 0.40532699  0.59467351]
[ 0.29368797  0.70631182]
[ 0.29453936  0.70546055]
[ 0.55783367  0.44216621]
[ 0.28325385  0.71674562]
[ 0.36064577  0.63935447]
[ 0.30611756  0.69388217]
[ 0.46120268  0.53879702]
[ 0.44514677  0.5548535 ]
[ 0.44462633  0.55537361]
[ 0.57767564  0.42232379]
[ 0.44297025  0.55702937]
[ 0.33757842  0.66242182]
[ 0.2295171   0.77048212]
[ 0.63497436  0.36502579]
[ 0.420546    0.57945412]
[ 0.66389906  0.33610052]
[ 0.60050613  0.39949384]
[ 0.85343266  0.14656924]
[ 0.27313551  0.72686446]
[ 0.61052877  0.38947126]
[ 0.33150259  0.66849715]
[ 0.49420217  0.50579756]
[ 0.99070418  0.00929891]
[ 0.41583848  0.5841617 ]
[ 0.59012026  0.40987983]
[ 0.53116566  0.46883476]
[ 0.77849859  0.22150087]
[ 0.47128227  0.52871788]
[ 0.59409463  0.40590578]
[  9.99986768e-01   1.38458581e-05]
[ 0.88710427  0.11290085]
[ 0.38258633  0.61741298]
[ 0.69684047  0.30315974]
[ 0.45512283  0.54487699]
[ 0.93217218  0.06782822]
[ 0.79686576  0.20313448]
[ 0.72071743  0.27928177]
[ 0.30953112  0.69046956]
[ 0.53170806  0.4682917 ]
[ 0.40988037  0.59011966]
[ 0.59999323  0.40000668]
[ 0.40251613  0.59748369]
[ 0.67142516  0.32857513]
[ 0.20156236  0.79843718]
[ 0.92152327  0.07847605]
[  9.99154031e-01   8.47858435e-04]
[ 0.52256912  0.47743055]
[ 0.4757039   0.52429605]
[ 0.41009262  0.58990765]
[ 0.33052737  0.66947293]
[ 0.55534691  0.44465327]
[ 0.51229066  0.48770943]
[ 0.84778225  0.15221657]
[ 0.55342585  0.44657403]
[ 0.65931183  0.34068784]
[ 0.78595084  0.21404926]
[ 0.7549836   0.24501581]
[ 0.40679184  0.59320801]
[ 0.50203538  0.49796498]
[ 0.54850274  0.4514975 ]
[ 0.82432592  0.17567439]
[ 0.59457749  0.40542221]
[ 0.8728568   0.12714298]
[ 0.98866463  0.01133599]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.82887065  0.17112929]
[ 0.72267377  0.27732581]
[ 0.33149639  0.66850418]
[ 0.59723371  0.40276676]
[ 0.63061678  0.36938342]
roc_auc for the hours: 0.375675675676
Accuracy validation for the hours:  0.518987341772
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=5']
Command line arguments: Namespace(channels=[5], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 13:34:16.322273
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.789049344
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 235.012228394 192.875953674
20 566.306152344 529.812561035
30 831.834051514 789.432739258
40 1104.28349609 1051.81738281
50 1420.10662842 1351.90142822
60 1824.84020996 1734.17260742
70 2397.64912109 2271.95996094
80 3280.87207031 3104.30908203
90 4885.48979492 4615.77099609
100 55591.203125 52662.2460938
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.14004[0m     [32m0.80929[0m    1.40870      0.49932  123.46s
      2     [36m0.65724[0m     [32m0.72524[0m    0.90624      0.54491  125.29s
      3     [36m0.63583[0m     [32m0.70930[0m    0.89642      0.54158  125.56s
      4     [36m0.60243[0m     0.72256    0.83375      0.52647  124.42s
      5     [36m0.55891[0m     0.74034    0.75494      0.51247  124.80s
      6     [36m0.51025[0m     0.78731    0.64809      0.54892  124.77s
      7     [36m0.44430[0m     0.83561    0.53171      0.53671  125.77s
Early stopping.
Best valid loss was 0.709299 at epoch 3.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.53968579235
Error rate (%):  46.031420765
yVal [0 1 1 ..., 1 1 0]
[[2686 4594]
 [2145 5215]]
roc_auc: 0.558072957104
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.33104742  0.66895258]
[ 0.17500651  0.82499343]
[ 0.51796693  0.48203287]
[ 0.46027783  0.53972179]
[ 0.43800214  0.56199801]
[ 0.49178436  0.50821561]
[  9.99432445e-01   5.67986572e-04]
[ 0.86133724  0.13866271]
[ 0.14570968  0.85429084]
[ 0.20866916  0.79133111]
[ 0.13880906  0.86119068]
[ 0.44793841  0.55206156]
[ 0.59006387  0.40993619]
[ 0.27382717  0.72617263]
[ 0.4971776  0.5028221]
[ 0.09429296  0.90570688]
[ 0.16958909  0.83041078]
[ 0.79171348  0.20828691]
[ 0.21080309  0.78919685]
[ 0.42045745  0.57954299]
[ 0.48657441  0.51342583]
[ 0.21049795  0.78950185]
[ 0.36906257  0.63093752]
[ 0.87687415  0.12312596]
[ 0.28422925  0.71577096]
[ 0.70597696  0.29402301]
[ 0.96660817  0.03339367]
[ 0.86254764  0.13745284]
[ 0.25886649  0.74113375]
[ 0.42451775  0.57548225]
[ 0.57538897  0.42461109]
[ 0.55109996  0.44889951]
[ 0.68775094  0.31224921]
[ 0.64852971  0.35147014]
[ 0.57756352  0.42243677]
[ 0.37028092  0.6297189 ]
[ 0.77613389  0.22386663]
[ 0.76900965  0.23099084]
[ 0.32605883  0.67394197]
[ 0.64294434  0.3570559 ]
[ 0.88136494  0.11862992]
[ 0.4602524   0.53974712]
[ 0.77712649  0.22287372]
[ 0.38581273  0.61418754]
[ 0.95384848  0.0461508 ]
[ 0.76922047  0.23077935]
[ 0.95815229  0.04184887]
[ 0.17333607  0.82666391]
[ 0.72752458  0.27247539]
[ 0.48354688  0.51645184]
[ 0.82980049  0.17019944]
[ 0.804811    0.19518958]
[ 0.98795962  0.01204166]
[ 0.20587645  0.79412353]
[ 0.81081951  0.18917997]
[ 0.94108081  0.05891965]
[ 0.86080939  0.13919197]
[ 0.55804414  0.44195566]
[ 0.5905506   0.40944973]
[ 0.85230386  0.14769632]
[ 0.6530022   0.34699714]
[ 0.84103459  0.15896626]
[ 0.8535527  0.146448 ]
[ 0.39200142  0.60799891]
[ 0.46613488  0.53386575]
[ 0.97869474  0.02130604]
[ 0.92420739  0.0757926 ]
[ 0.80174679  0.19825403]
[ 0.50594366  0.49405658]
[ 0.74142838  0.25857118]
[ 0.88430268  0.11569694]
[ 0.22641386  0.77358609]
[ 0.99465382  0.0053477 ]
[ 0.89285511  0.10714487]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.69944227  0.30055815]
[ 0.20629549  0.79370487]
[ 0.53940606  0.46059409]
[ 0.45330435  0.54669583]
[ 0.81592935  0.18407154]
roc_auc for the hours: 0.564864864865
Accuracy validation for the hours:  0.569620253165
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=6']
Command line arguments: Namespace(channels=[6], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 13:56:12.268237
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.789204992
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 202.214730835 163.390800476
20 485.357543945 447.911590576
30 715.467364502 669.537719727
40 956.924304199 896.82824707
50 1245.06646729 1163.88885498
60 1624.86760254 1511.64050293
70 2166.63815918 2006.6552124
80 3004.99970703 2778.06762695
90 4614.8956543 4242.0456543
100 66751.359375 59361.5703125
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.11064[0m     [32m0.73446[0m    1.51219      0.54713  122.55s
      2     [36m0.64023[0m     [32m0.65371[0m    0.97938      0.60365  125.01s
      3     [36m0.61384[0m     [32m0.64508[0m    0.95158      0.62483  124.59s
      4     [36m0.57989[0m     0.65566    0.88443      0.60972  124.12s
      5     [36m0.52798[0m     0.69645    0.75811      0.55601  124.51s
      6     [36m0.46524[0m     0.80529    0.57773      0.57804  125.82s
      7     [36m0.40431[0m     0.86247    0.46878      0.56685  126.20s
Early stopping.
Best valid loss was 0.645077 at epoch 3.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.620081967213
Error rate (%):  37.9918032787
yVal [0 1 1 ..., 1 1 0]
[[3744 3536]
 [2026 5334]]
roc_auc: 0.626976379599
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.60502136  0.39497823]
[ 0.33576924  0.66423088]
[ 0.75120914  0.24879168]
[ 0.94318146  0.05681838]
[ 0.75885433  0.24114554]
[ 0.38098025  0.61901969]
[  1.00000000e+00   4.53326543e-11]
[ 0.83973366  0.16026676]
[ 0.38648507  0.61351436]
[ 0.45534799  0.54465187]
[ 0.4290348   0.57096541]
[ 0.92172933  0.0782707 ]
[ 0.83625817  0.1637418 ]
[ 0.68097085  0.31902963]
[ 0.93400067  0.06600007]
[ 0.52132994  0.47867   ]
[ 0.3032988   0.69670159]
[ 0.99820703  0.00179394]
[ 0.25834227  0.74165773]
[ 0.83651161  0.16348819]
[ 0.41426641  0.58573306]
[ 0.81185526  0.18814443]
[ 0.35551521  0.64448416]
[ 0.98735821  0.01264259]
[ 0.43380895  0.56619132]
[ 0.87164676  0.12835306]
[ 0.9689396   0.03106248]
[ 0.95744127  0.04255975]
[ 0.63344944  0.36655062]
[ 0.81813818  0.18186186]
[ 0.76124281  0.23875675]
[ 0.87259245  0.12740773]
[  9.99714136e-01   2.86793278e-04]
[ 0.7145105   0.28548911]
[ 0.97307539  0.0269262 ]
[ 0.44483361  0.55516648]
[ 0.77790266  0.22209747]
[ 0.84877455  0.15122508]
[ 0.64875305  0.35124627]
[  9.99989152e-01   1.16638148e-05]
[ 0.87695384  0.12304149]
[ 0.49688098  0.50311875]
[  9.99387562e-01   6.13252341e-04]
[ 0.59438872  0.4056111 ]
[ 0.99285692  0.00714357]
[ 0.7808758  0.2191246]
[ 0.97079313  0.02920774]
[ 0.64008564  0.3599138 ]
[ 0.91264075  0.08736094]
[ 0.47722253  0.5227775 ]
[ 0.9606424   0.03935785]
[ 0.70357877  0.29642132]
[ 0.91942805  0.08057272]
[ 0.7445448   0.25545502]
[ 0.96448314  0.03551682]
[ 0.99778736  0.00221427]
[ 0.93137044  0.06863007]
[ 0.24099322  0.75900674]
[ 0.33387393  0.66612571]
[ 0.51318067  0.48681912]
[ 0.37147263  0.62852705]
[ 0.99877405  0.0012276 ]
[ 0.83796239  0.162038  ]
[ 0.4416723  0.5583275]
[ 0.7956937  0.2043066]
[ 0.99603444  0.00396567]
[ 0.89891058  0.10108936]
[ 0.87005419  0.12994659]
[ 0.85784113  0.14215907]
[ 0.66351652  0.33648351]
[ 0.94355029  0.05645084]
[ 0.74174339  0.25825647]
[  9.99971688e-01   2.95283098e-05]
[ 0.99036175  0.00963881]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.68164361  0.31835636]
[ 0.56968224  0.4303177 ]
[ 0.66834158  0.33165908]
[ 0.7457642   0.25423563]
[ 0.84365374  0.15634678]
roc_auc for the hours: 0.616216216216
Accuracy validation for the hours:  0.721518987342
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=7']
Command line arguments: Namespace(channels=[7], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 14:18:05.524237
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.788979712
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 185.573376465 149.823059082
20 446.898638916 411.701080322
30 662.398498535 618.625640869
40 892.103625488 835.423828125
50 1170.04150391 1093.59197998
60 1538.95534668 1432.44165039
70 2070.14841309 1921.18255615
80 2918.14931641 2705.77734375
90 4599.92104492 4269.7043457
100 65353.1796875 57503.8945312
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m1.09631[0m     [32m0.68848[0m    1.59235      0.61467  123.29s
      2     [36m0.61385[0m     [32m0.61469[0m    0.99863      0.64370  125.23s
      3     [36m0.59039[0m     0.63181    0.93444      0.64302  125.04s
      4     [36m0.56952[0m     0.64862    0.87805      0.63422  125.27s
      5     [36m0.55395[0m     0.68825    0.80487      0.62372  125.23s
      6     [36m0.51262[0m     0.73229    0.70003      0.60758  125.34s
Early stopping.
Best valid loss was 0.614693 at epoch 2.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.646584699454
Error rate (%):  35.3415300546
yVal [0 1 1 ..., 1 1 0]
[[3685 3595]
 [1579 5781]]
roc_auc: 0.693348774188
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.8656919   0.13430823]
[ 0.6382972   0.36170271]
[ 0.99764401  0.0023565 ]
[ 0.95648992  0.0435099 ]
[ 0.51152718  0.48847279]
[ 0.5332346   0.46676549]
[  1.00000000e+00   1.18874285e-10]
[ 0.86058724  0.13941169]
[ 0.83438218  0.16561846]
[ 0.819713    0.18028745]
[ 0.72019798  0.2798011 ]
[ 0.98246789  0.01753299]
[ 0.77968073  0.22031915]
[ 0.75013119  0.24986929]
[ 0.66861922  0.33138135]
[ 0.82275909  0.17724086]
[ 0.31400678  0.6859929 ]
[ 0.29548344  0.70451683]
[ 0.36435774  0.63564211]
[ 0.75550991  0.24449018]
[ 0.50430262  0.49569711]
[ 0.3628197   0.63718075]
[ 0.50108558  0.49891499]
[ 0.96336687  0.03663328]
[ 0.69149911  0.30850118]
[ 0.79536605  0.20463446]
[ 0.92490846  0.07509255]
[ 0.99349874  0.00650288]
[ 0.95797354  0.04202694]
[ 0.92982596  0.07017508]
[ 0.75461316  0.24538679]
[ 0.83070797  0.16929322]
[ 0.94259703  0.05740321]
[ 0.50866205  0.49133837]
[ 0.96945471  0.0305454 ]
[ 0.4634445   0.53655577]
[ 0.77774161  0.22225365]
[  9.99106944e-01   8.94648721e-04]
[ 0.57115191  0.42884794]
[ 0.86809754  0.13190223]
[ 0.88528186  0.11472208]
[ 0.64487815  0.35512093]
[ 0.98186678  0.01813396]
[ 0.64848274  0.35151696]
[ 0.95565683  0.0443433 ]
[ 0.79260975  0.20739022]
[ 0.31089512  0.68910521]
[ 0.25788218  0.74211836]
[ 0.65888321  0.34111717]
[ 0.17146225  0.82853794]
[ 0.67361969  0.32638061]
[ 0.51110977  0.48889014]
[ 0.93517011  0.06482981]
[ 0.35173738  0.64826328]
[ 0.8434881   0.15651189]
[ 0.99826628  0.00173393]
[ 0.84615052  0.15385026]
[ 0.49562088  0.50437903]
[ 0.70827621  0.29172325]
[ 0.61282772  0.38717261]
[ 0.17293064  0.82706958]
[ 0.14845051  0.85154933]
[ 0.84551829  0.15447876]
[ 0.62479627  0.37520292]
[ 0.37062049  0.62937969]
[ 0.49054402  0.5094558 ]
[ 0.43910539  0.56089401]
[ 0.73900008  0.26100037]
[ 0.65516782  0.34483156]
[ 0.28725615  0.71274358]
[ 0.65566236  0.34433791]
[ 0.76788247  0.23211706]
[  9.99287069e-01   7.13614456e-04]
[ 0.99125457  0.00874559]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.75785387  0.24214584]
[ 0.04195691  0.95804447]
[ 0.4480634   0.55193597]
[ 0.65601403  0.34398469]
[ 0.5301289   0.46987104]
roc_auc for the hours: 0.718918918919
Accuracy validation for the hours:  0.759493670886
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=8']
Command line arguments: Namespace(channels=[8], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 14:37:45.219680
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.788824064
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 182.433731079 149.94278717
20 436.049041748 410.029296875
30 635.802856445 606.535400391
40 835.57677002 801.683410645
50 1056.74621582 1016.02960205
60 1321.59230957 1272.10205078
70 1665.66821289 1603.8125
80 2168.0465332 2087.17016602
90 3108.63288574 2996.27380371
100 59862.1054688 57871.4179688
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.93987[0m     [32m0.64694[0m    1.45278      0.59375  118.50s
      2     [36m0.63577[0m     0.65116    0.97636      0.56421  120.12s
      3     [36m0.60829[0m     0.71233    0.85395      0.47328  120.64s
      4     [36m0.55330[0m     0.84060    0.65822      0.47618  120.69s
      5     [36m0.48367[0m     1.03995    0.46508      0.42000  120.25s
Early stopping.
Best valid loss was 0.646945 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.591598360656
Error rate (%):  40.8401639344
yVal [0 1 1 ..., 1 1 0]
[[2696 4584]
 [1395 5965]]
roc_auc: 0.609499662192
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.25599465  0.74400628]
[ 0.48115003  0.51884985]
[ 0.86902207  0.13097778]
[ 0.24507612  0.75492358]
[ 0.61502057  0.38497934]
[ 0.41546378  0.58453578]
[ 0.97036052  0.02963932]
[ 0.35922873  0.64077151]
[ 0.24215724  0.75784278]
[ 0.77701366  0.22298664]
[ 0.63647848  0.36352158]
[ 0.82010865  0.17989187]
[ 0.94241023  0.05759062]
[ 0.4569793   0.54302055]
[ 0.59855533  0.40144506]
[ 0.73717701  0.26282394]
[ 0.12947886  0.87052149]
[ 0.17774208  0.82225782]
[ 0.39937183  0.60062802]
[ 0.89507771  0.10492224]
[ 0.66177636  0.33822384]
[ 0.60670078  0.3932988 ]
[ 0.64618802  0.35381216]
[ 0.94431949  0.05568083]
[ 0.29101291  0.70898712]
[ 0.9458735   0.05412642]
[ 0.97184587  0.02815516]
[ 0.95090061  0.04909964]
[ 0.28810659  0.7118929 ]
[ 0.59178782  0.40821278]
[ 0.39011744  0.60988259]
[ 0.37551871  0.6244815 ]
[ 0.99705476  0.0029456 ]
[ 0.25909427  0.74090505]
[ 0.6079011   0.39209896]
[ 0.19806021  0.80194002]
[ 0.8125506   0.18744844]
[ 0.27293286  0.72706711]
[ 0.26496047  0.73504066]
[  9.99999523e-01   5.40849271e-07]
[ 0.78823608  0.21177016]
[ 0.14092407  0.8590768 ]
[ 0.24786597  0.75213391]
[ 0.21783578  0.78216439]
[ 0.61889666  0.38110408]
[ 0.74003512  0.25996518]
[ 0.05550545  0.94449449]
[ 0.30464673  0.69535279]
[ 0.56179029  0.43821052]
[ 0.53280765  0.46719283]
[ 0.06749479  0.93250507]
[ 0.07190738  0.92809314]
[ 0.0323434   0.96765608]
[ 0.19652876  0.80347043]
[ 0.95726234  0.04273759]
[ 0.96913683  0.03086575]
[ 0.13808744  0.86191165]
[ 0.03173558  0.9682638 ]
[ 0.04439564  0.9556042 ]
[ 0.01968359  0.98031652]
[ 0.05065536  0.94934452]
[ 0.99774539  0.00225524]
[ 0.76384652  0.23615374]
[ 0.12065163  0.8793484 ]
[ 0.04384669  0.95615315]
[ 0.01918585  0.98081398]
[ 0.04494623  0.9550541 ]
[ 0.05238368  0.94761604]
[ 0.01852838  0.98147255]
[ 0.07305503  0.92694461]
[ 0.10357687  0.89642251]
[ 0.28916472  0.71083474]
[ 0.23494381  0.76505619]
[ 0.9879756  0.0120248]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.43724471  0.56275511]
[ 0.42386401  0.57613587]
[ 0.41506359  0.5849362 ]
[ 0.14008388  0.85991645]
[ 0.04899082  0.95100874]
roc_auc for the hours: 0.608108108108
Accuracy validation for the hours:  0.46835443038
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=9']
Command line arguments: Namespace(channels=[9], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 14:54:44.226606
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.789897216
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 3.73858091831 3.27668011189
20 8.95421485901 8.65685558319
30 13.5715697289 13.20033741
40 19.6790660858 19.0048999786
50 43.6948184967 34.7788333893
60 449.341595459 388.074157715
70 780.715905762 715.159332275
80 1201.33637695 1117.43395996
90 1923.547229 1805.91333008
100 43853.8789062 40357.3945312
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.82086[0m     [32m0.68963[0m    1.19028      0.45244  118.10s
      2     [36m0.67844[0m     0.69526    0.97581      0.48523  120.37s
      3     [36m0.65754[0m     0.75717    0.86842      0.52818  120.76s
      4     [36m0.61872[0m     0.88287    0.70080      0.46901  120.48s
      5     [36m0.56462[0m     1.03960    0.54311      0.46252  120.05s
Early stopping.
Best valid loss was 0.689631 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.444603825137
Error rate (%):  55.5396174863
yVal [0 1 1 ..., 1 1 0]
[[2898 4382]
 [3749 3611]]
roc_auc: 0.461853873029
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.82515341  0.17484672]
[ 0.7772094   0.22279111]
[ 0.81449658  0.18550402]
[ 0.96048057  0.03951911]
[ 0.8338663   0.16613337]
[ 0.87501848  0.12498135]
[ 0.48408601  0.51591396]
[ 0.47539237  0.52460766]
[ 0.47584364  0.52415645]
[ 0.47548336  0.52451646]
[ 0.4717783   0.52822191]
[ 0.47842494  0.52157521]
[ 0.47413018  0.52586979]
[ 0.47742403  0.52257615]
[ 0.47425678  0.52574295]
[ 0.47468454  0.5253157 ]
[ 0.47445455  0.52554536]
[ 0.49023482  0.50976497]
[ 0.47897834  0.5210219 ]
[ 0.4745836   0.52541649]
[ 0.4766354   0.52336419]
[ 0.47870773  0.52129233]
[ 0.47794712  0.522053  ]
[ 0.47775102  0.52224922]
[ 0.47853839  0.52146173]
[ 0.48051104  0.51948887]
[ 0.47495529  0.52504462]
[ 0.47724026  0.52275974]
[ 0.48266321  0.51733679]
[ 0.4781968   0.52180332]
[ 0.47650185  0.52349836]
[ 0.48028418  0.51971591]
[ 0.50614971  0.49385023]
[ 0.48275825  0.51724207]
[ 0.47940427  0.52059561]
[ 0.48212087  0.51787913]
[ 0.47921154  0.52078855]
[ 0.4874171   0.51258284]
[ 0.47541586  0.52458441]
[ 0.53627932  0.46372074]
[ 0.47747809  0.52252185]
[ 0.47673225  0.5232681 ]
[ 0.53759152  0.46240842]
[ 0.76704556  0.23295444]
[ 0.64382583  0.35617331]
[ 0.72886753  0.27113277]
[ 0.94533813  0.05466177]
[ 0.65096885  0.34903142]
[ 0.51768404  0.48231584]
[ 0.76905525  0.2309456 ]
[ 0.92901182  0.0709879 ]
[ 0.86557198  0.13442713]
[ 0.79679006  0.20321035]
[ 0.78189492  0.21810532]
[ 0.95980591  0.04019389]
[ 0.97575098  0.0242503 ]
[ 0.82217252  0.17782773]
[ 0.95156634  0.0484345 ]
[ 0.94844747  0.05155241]
[ 0.93439281  0.06560654]
[ 0.96842158  0.03157941]
[ 0.81183124  0.18816903]
[ 0.75869912  0.24130103]
[ 0.8844434   0.11555591]
[ 0.91210496  0.08789497]
[ 0.9753024   0.02469716]
[ 0.79991376  0.2000864 ]
[ 0.86921275  0.13078743]
[ 0.81145245  0.18854822]
[ 0.56292224  0.43707794]
[ 0.74962062  0.25037891]
[ 0.68600386  0.31399673]
[ 0.96677488  0.03322452]
[ 0.98792648  0.01207351]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.82618374  0.17381628]
[ 0.4775984   0.52240163]
[ 0.47894999  0.5210498 ]
[ 0.75727075  0.24272913]
[ 0.93915331  0.06084663]
roc_auc for the hours: 0.448648648649
Accuracy validation for the hours:  0.53164556962
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=10']
Command line arguments: Namespace(channels=[10], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 15:11:44.933466
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.78913536
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 184.185713196 152.952148438
20 440.024975586 416.685943604
30 641.064581299 616.230438232
40 840.985620117 812.821044922
50 1060.82244873 1027.43164062
60 1320.46096191 1279.14282227
70 1649.28624268 1598.09313965
80 2109.55322266 2045.45068359
90 2906.99501953 2825.17199707
100 41643.4882812 36225.46875
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.93988[0m     [32m0.69496[0m    1.35241      0.55686  117.62s
      2     [36m0.61467[0m     1.01674    0.60454      0.46832  120.08s
      3     [36m0.57459[0m     0.83411    0.68886      0.46337  120.39s
      4     [36m0.53993[0m     0.92663    0.58268      0.40019  120.22s
      5     [36m0.50823[0m     0.95718    0.53096      0.46158  120.24s
Early stopping.
Best valid loss was 0.694963 at epoch 1.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.555737704918
Error rate (%):  44.4262295082
yVal [0 1 1 ..., 1 1 0]
[[1247 6033]
 [ 471 6889]]
roc_auc: 0.551953638244
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.1017193   0.89828074]
[ 0.04942791  0.95057219]
[ 0.20457992  0.79542005]
[ 0.00950146  0.99049801]
[ 0.04669269  0.95330757]
[ 0.1242523  0.8757481]
[ 0.74635589  0.25364423]
[ 0.13013247  0.86986679]
[ 0.07894915  0.92105043]
[ 0.26399454  0.73600554]
[ 0.12315667  0.87684304]
[ 0.15192765  0.84807247]
[ 0.14548662  0.85451341]
[ 0.03630193  0.96369821]
[ 0.33616146  0.66383761]
[ 0.10755707  0.8924436 ]
[ 0.07583724  0.92416281]
[ 0.03824274  0.96175671]
[ 0.03713785  0.96286219]
[ 0.32914582  0.67085403]
[ 0.14001054  0.85999006]
[ 0.15256308  0.84743702]
[ 0.35899404  0.64100605]
[ 0.60054678  0.39945298]
[ 0.0112928   0.98870772]
[ 0.30355376  0.69644666]
[ 0.60865051  0.39134973]
[ 0.6819064  0.3180936]
[ 0.0937851   0.90621471]
[ 0.0318808   0.96811903]
[ 0.03927806  0.96072292]
[ 0.0505906   0.94940943]
[ 0.98119372  0.01880803]
[ 0.07707766  0.9229219 ]
[ 0.310911    0.68908828]
[ 0.1532535  0.8467468]
[ 0.74796391  0.25203696]
[ 0.11445078  0.88554937]
[ 0.25126344  0.74873549]
[  9.99974370e-01   2.56000294e-05]
[ 0.53651297  0.46348736]
[ 0.13268872  0.86731058]
[ 0.25941628  0.74058354]
[ 0.1191577   0.88084191]
[ 0.59138781  0.40861273]
[ 0.6287756   0.37122437]
[ 0.01847393  0.9815259 ]
[ 0.20923999  0.7907595 ]
[ 0.35135594  0.64864409]
[ 0.52348715  0.47651282]
[ 0.00391839  0.99608141]
[ 0.05597797  0.94402248]
[ 0.01468132  0.9853183 ]
[ 0.19154623  0.80845368]
[ 0.9201839   0.07981591]
[ 0.96030647  0.03969517]
[ 0.08977233  0.91022736]
[ 0.05511441  0.94488549]
[ 0.06159693  0.93840212]
[ 0.00853003  0.99147016]
[ 0.01339928  0.98660111]
[ 0.42112535  0.57887471]
[ 0.65547049  0.34452966]
[ 0.11789457  0.88210487]
[ 0.04215423  0.95784563]
[ 0.01489057  0.98510945]
[ 0.07827009  0.92172915]
[ 0.0144165   0.98558336]
[ 0.06912214  0.93087876]
[ 0.06988403  0.9301157 ]
[ 0.16464131  0.83535904]
[ 0.40155452  0.59844506]
[ 0.00730383  0.99269569]
[ 0.98779345  0.01220844]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.06023576  0.93976468]
[ 0.30521312  0.6947872 ]
[ 0.31605935  0.68394119]
[ 0.10841492  0.89158499]
[ 0.00623636  0.99376255]
roc_auc for the hours: 0.57027027027
Accuracy validation for the hours:  0.253164556962
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=11']
Command line arguments: Namespace(channels=[11], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 15:28:43.565325
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.7887872
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 175.594898987 145.517463684
20 419.360888672 395.677886963
30 609.753845215 583.460235596
40 797.140246582 765.498535156
50 1000.32189941 962.170196533
60 1235.86208496 1189.52197266
70 1531.12805176 1473.54846191
80 1943.29196777 1868.76367188
90 2665.28051758 2562.52746582
100 34311.5273438 30092.4199219
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.89346[0m     [32m0.79799[0m    1.11963      0.55405  118.72s
      2     [36m0.55461[0m     0.81567    0.67995      0.47848  121.28s
      3     [36m0.52934[0m     [32m0.79103[0m    0.66918      0.49872  121.01s
      4     [36m0.49115[0m     0.94807    0.51805      0.41837  120.99s
      5     [36m0.45689[0m     1.22801    0.37206      0.40352  121.25s
      6     [36m0.40381[0m     1.00257    0.40278      0.48318  121.08s
      7     [36m0.36135[0m     1.45986    0.24752      0.45321  121.10s
Early stopping.
Best valid loss was 0.791026 at epoch 3.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.515095628415
Error rate (%):  48.4904371585
yVal [0 1 1 ..., 1 1 0]
[[3458 3822]
 [3277 4083]]
roc_auc: 0.522558416448
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.32463887  0.67536139]
[ 0.4102011   0.58979791]
[ 0.93276489  0.0672359 ]
[ 0.00702524  0.99297726]
[ 0.60236156  0.39763793]
[ 0.0283736   0.97162682]
[  1.00000000e+00   1.28126831e-09]
[ 0.47667363  0.5233264 ]
[ 0.44012603  0.55987394]
[ 0.88604409  0.11395624]
[ 0.47316518  0.52683485]
[  9.99943256e-01   5.74135920e-05]
[ 0.76527625  0.2347236 ]
[ 0.22362439  0.77637601]
[ 0.80616856  0.19383228]
[ 0.64043272  0.35956776]
[ 0.03174032  0.96825999]
[ 0.06228892  0.93771249]
[ 0.32344577  0.67655438]
[ 0.85733849  0.14266209]
[ 0.79518902  0.20481163]
[ 0.05051959  0.94948226]
[ 0.2339268  0.7660737]
[  1.00000000e+00   9.54143448e-12]
[ 0.00202455  0.99797773]
[ 0.95481098  0.04519016]
[ 0.99182945  0.00817053]
[  1.00000000e+00   1.18247079e-09]
[ 0.0367412   0.96325874]
[ 0.04770366  0.95229733]
[ 0.78575504  0.21424566]
[ 0.20748276  0.79251796]
[ 0.71613461  0.28386521]
[ 0.70622021  0.29377997]
[ 0.92640275  0.07359741]
[ 0.82458866  0.17541167]
[ 0.79978347  0.20021148]
[ 0.9863708   0.01362951]
[ 0.14530344  0.85469776]
[ 0.99534136  0.00465897]
[ 0.81396699  0.18603049]
[ 0.07523448  0.92476511]
[ 0.98356038  0.01644034]
[ 0.65956599  0.34043533]
[ 0.68137324  0.31862667]
[ 0.7790606   0.22093928]
[ 0.28319713  0.7168026 ]
[ 0.41925886  0.58074111]
[ 0.84285563  0.1571447 ]
[ 0.48302928  0.51697123]
[ 0.01753625  0.98246366]
[ 0.58978713  0.41021207]
[ 0.83493716  0.16506357]
[ 0.16994108  0.83005959]
[ 0.86930579  0.13069397]
[ 0.90832198  0.09167814]
[ 0.29984656  0.70015424]
[ 0.07830861  0.92169172]
[ 0.03380862  0.96619195]
[ 0.04711333  0.95288628]
[ 0.02899697  0.97100365]
[ 0.99856752  0.00143287]
[ 0.83084697  0.16914943]
[ 0.39896002  0.60104078]
[ 0.24921416  0.75078601]
[ 0.00203891  0.99796337]
[ 0.26472187  0.7352792 ]
[ 0.59527051  0.40473014]
[ 0.69078261  0.30921721]
[ 0.701096    0.29890406]
[ 0.46037814  0.53962219]
[ 0.48667485  0.51332468]
[ 0.71893281  0.28106794]
[ 0.9807151   0.01928504]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.49404323  0.50595683]
[ 0.4593749   0.54062474]
[ 0.50257003  0.49742934]
[ 0.51148564  0.48851433]
[ 0.87190336  0.12809654]
roc_auc for the hours: 0.475675675676
Accuracy validation for the hours:  0.518987341772
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=12']
Command line arguments: Namespace(channels=[12], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 15:49:50.760877
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Memory usage (GB): 0.788799488
train_counter_seizure 20 val_counter_seizure 5
train_counter_normal 292 val_counter_normal 74
percentiles:
0 0.0 0.0
10 197.847105408 162.232131958
20 466.161206055 435.430114746
30 668.632995605 633.256835938
40 862.47142334 820.440124512
50 1068.59735107 1017.74472046
60 1307.13903809 1244.34411621
70 1610.01538086 1529.31481934
80 2051.1934082 1942.77392578
90 2874.82258301 2724.64428711
100 68176.640625 59867.6914062
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
size 73200
no_normal 366
no_seizure 25
no_samples_normal_ph 100
no_samples_seizure_ph 1464
Histogram:
[36600 36600]
magnitudes.shape (73200,)
labels.shape (73200,)
Ratio validation: 0.2
xVal.shape (14640,)
yVal.shape (14640,)
xTrain.shape (58560,)
yTrain.shape (58560,)
xVal.shape (14640, 2)
yVal.shape (14640,)
Saving preprocessed data...
Building models ...
Model name for the training phase:  net5
Model name for the evaluation phase:  net5
Training model...
# Neural Network with 149506 learnable parameters

## Layer information

  #    name  size
---  ------  ---------
  0          1x192x64
  1          64x192x64
  2          64x192x64
  3          64x192x64
  4          64x96x32
  5          64x96x32
  6          64x96x32
  7          64x48x16
  8          2x48x16
  9          2
 10          2

  epoch    trn loss    val loss    trn/val    valid acc  dur
-------  ----------  ----------  ---------  -----------  -------
      1     [36m0.90444[0m     [32m0.86412[0m    1.04666      0.53304  118.31s
      2     [36m0.60852[0m     [32m0.78240[0m    0.77776      0.57223  121.24s
      3     [36m0.57086[0m     0.83425    0.68428      0.58649  121.48s
      4     [36m0.52967[0m     0.81080    0.65326      0.58128  121.67s
      5     [36m0.48535[0m     [32m0.76086[0m    0.63790      0.59315  121.24s
      6     [36m0.41875[0m     0.95003    0.44077      0.55260  121.62s
      7     [36m0.36587[0m     0.92181    0.39690      0.54918  121.21s
      8     [36m0.29693[0m     1.03990    0.28554      0.53868  121.70s
      9     [36m0.24757[0m     1.36673    0.18114      0.50504  121.10s
Early stopping.
Best valid loss was 0.760858 at epoch 5.
Loaded parameters to layer 'conv2d1' (shape 64x1x3x3).
Loaded parameters to layer 'conv2d1' (shape 64).
Loaded parameters to layer 'conv2d2' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d2' (shape 64).
Loaded parameters to layer 'conv2d4' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d4' (shape 64).
Loaded parameters to layer 'conv2d5' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d5' (shape 64).
Loaded parameters to layer 'conv2d7' (shape 64x64x3x3).
Loaded parameters to layer 'conv2d7' (shape 64).
Loaded parameters to layer 'conv2d8' (shape 2x64x3x3).
Loaded parameters to layer 'conv2d8' (shape 2).
Saving model...
Validating...
probabilities.shape (14640, 2)
Showing last 30 test samples..
Predictions:
[1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0]
Ground Truth:
[1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0]
Performance on relevant data
Accuracy validation:  0.590163934426
Error rate (%):  40.9836065574
yVal [0 1 1 ..., 1 1 0]
[[3297 3983]
 [2017 5343]]
roc_auc: 0.614180424704
Changing batch iterator test:
Calculating final prediction for the hour long sessions
magnitudes_normal_val.shape (74, 1, 1406, 64)
[ 0.48230693  0.51769298]
[ 0.66410136  0.33589819]
[ 0.84847057  0.15153033]
[ 0.68236202  0.31763741]
[ 0.68043804  0.31956208]
[ 0.63595909  0.36404067]
[  1.00000000e+00   1.41114331e-09]
[ 0.43142316  0.56857741]
[ 0.69446993  0.30553073]
[ 0.47494131  0.52505881]
[ 0.54363066  0.45636931]
[ 0.88415366  0.11584691]
[ 0.71534657  0.28465328]
[ 0.70475084  0.29524925]
[ 0.7780472   0.22195303]
[ 0.62609273  0.37390804]
[ 0.26817769  0.73182273]
[ 0.52990478  0.47009557]
[ 0.55559105  0.44440892]
[ 0.64314574  0.35685411]
[ 0.70235234  0.29764825]
[ 0.40321928  0.5967809 ]
[ 0.58696413  0.41303617]
[ 0.98116732  0.0188327 ]
[ 0.25632855  0.74367166]
[ 0.91383862  0.08616227]
[ 0.93209302  0.06790631]
[ 0.99544603  0.00455378]
[ 0.12328278  0.87671751]
[ 0.27603558  0.72396475]
[ 0.54790878  0.45209092]
[ 0.18042293  0.81957793]
[  9.99988019e-01   1.19141041e-05]
[ 0.79267013  0.20733008]
[ 0.69332582  0.30667427]
[ 0.56880897  0.43119082]
[ 0.77364236  0.22635661]
[ 0.8885873   0.11141258]
[ 0.43934035  0.56066   ]
[  1.00000000e+00   6.45840240e-28]
[ 0.87713939  0.12286551]
[ 0.53878999  0.46121031]
[ 0.68626213  0.31373787]
[ 0.92519218  0.07480879]
[ 0.32101893  0.67898089]
[ 0.77784991  0.22215083]
[ 0.42610326  0.57389647]
[ 0.53244662  0.46755326]
[ 0.87516606  0.12483475]
[ 0.68313152  0.31686851]
[ 0.29959595  0.70040441]
[ 0.25881377  0.7411868 ]
[ 0.47983965  0.52016026]
[ 0.25874001  0.74125975]
[ 0.99105579  0.00894421]
[  1.00000000e+00   1.35044341e-12]
[ 0.43253374  0.56746608]
[ 0.58528459  0.41471544]
[ 0.39311832  0.60688156]
[ 0.41141859  0.58858156]
[ 0.38444632  0.61555403]
[ 0.57219428  0.42780471]
[ 0.85133272  0.14866626]
[ 0.3111583   0.68884224]
[ 0.52036721  0.4796333 ]
[ 0.11429001  0.88571042]
[ 0.35376859  0.64623171]
[ 0.65387702  0.34612307]
[ 0.64176321  0.35823661]
[ 0.64934105  0.35065925]
[ 0.57299656  0.42700347]
[ 0.40261993  0.59737986]
[ 0.71542966  0.28457025]
[ 0.99350607  0.00649396]
magnitudes_seizure_val.shape (5, 1, 1406, 64)
[ 0.70777696  0.2922222 ]
[ 0.22397071  0.77602923]
[ 0.33662117  0.66337919]
[ 0.58413583  0.41586456]
[ 0.73406917  0.26593107]
roc_auc for the hours: 0.589189189189
Accuracy validation for the hours:  0.658227848101
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=13']
Command line arguments: Namespace(channels=[13], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 16:15:03.118701
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Read in dataset from test_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/test_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=14']
Command line arguments: Namespace(channels=[14], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 16:18:56.038662
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Read in dataset from test_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/test_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
['python', 'train_batch.py', '--debug-sub-ratio=1', '--model-filename=run2.pickle', '--config-filename=config.yml', '--no-channels=1', '--target-gpu=gpu1', '--data-path=/home/eavsteen/seizure_detection/data', '--patients=patient1', '--channels=15']
Command line arguments: Namespace(channels=[15], chosen_validation_ratio=0.2, config_filename='config.yml', data_path='/home/eavsteen/seizure_detection/data', debug_sub_ratio=1.0, exclude_user=[], fixed_seed=True, include_user=[], mode='None', model_filename='run2.pickle', no_channels=1, no_preprocessing=False, no_save_model=False, no_save_preprocessed=False, no_training=False, patients=['patient1'], plot_prob_dist=False, shift=0, shuffle_before_split=True, target_gpu='gpu1')
Git reference:  Timestamp: 2016-09-28 16:23:15.466066
Hostname: kat
gpu1
Configuration 'config.yml':
Configuration:
preprocess:
    floor: 0
    ceil: 64
    fft_width: 1024
    overlap: 0
    magnitude_window: 192
    augmentation: shift
    normalization: min_max_x255
    include_userdata: false
training:
    model: net5
evaluation:
    online_training: False
    model: net5

end Configuration
Loading and preprocessing data...
test_magnitude.shape (1406, 64)
Read in dataset from train_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/train_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
normal i 1
normal i 2
normal i 3
normal i 4
normal i 5
normal i 6
normal i 7
normal i 8
normal i 9
normal i 10
normal i 11
normal i 12
normal i 13
normal i 14
normal i 15
normal i 16
normal i 17
normal i 18
normal i 19
normal i 20
normal i 21
normal i 22
normal i 23
normal i 24
normal i 25
normal i 26
normal i 27
normal i 28
normal i 29
normal i 30
normal i 31
normal i 32
normal i 33
normal i 34
normal i 35
normal i 36
normal i 37
normal i 38
normal i 39
normal i 40
normal i 41
normal i 42
normal i 43
normal i 44
normal i 45
normal i 46
normal i 47
normal i 48
normal i 49
normal i 50
normal i 51
normal i 52
normal i 53
normal i 54
normal i 55
normal i 56
normal i 57
normal i 58
normal i 59
normal i 60
normal i 61
normal i 62
normal i 63
normal i 64
normal i 65
normal i 66
normal i 67
normal i 68
normal i 69
normal i 70
normal i 71
normal i 72
normal i 73
normal i 74
normal i 75
normal i 76
normal i 77
normal i 78
normal i 79
normal i 80
normal i 81
normal i 82
normal i 83
normal i 84
normal i 85
normal i 86
normal i 87
normal i 88
normal i 89
normal i 90
normal i 91
normal i 92
normal i 93
normal i 94
normal i 95
normal i 96
normal i 97
normal i 98
normal i 99
normal i 100
normal i 101
normal i 102
normal i 103
normal i 104
normal i 105
normal i 106
normal i 107
normal i 108
normal i 109
normal i 110
normal i 111
normal i 112
normal i 113
normal i 114
normal i 115
normal i 116
normal i 117
normal i 118
normal i 119
normal i 120
normal i 121
normal i 122
normal i 123
normal i 124
normal i 125
normal i 126
normal i 127
normal i 128
normal i 129
normal i 130
normal i 131
normal i 132
normal i 133
normal i 134
normal i 135
normal i 136
normal i 137
normal i 138
normal i 139
normal i 140
normal i 141
normal i 142
normal i 143
normal i 144
normal i 145
normal i 146
normal i 147
normal i 148
normal i 149
normal i 150
normal i 151
normal i 152
normal i 153
normal i 154
normal i 155
normal i 156
normal i 157
normal i 158
normal i 159
normal i 160
normal i 161
normal i 162
normal i 163
normal i 164
normal i 165
normal i 166
normal i 167
normal i 168
normal i 169
normal i 170
normal i 171
normal i 172
normal i 173
normal i 174
normal i 175
normal i 176
normal i 177
normal i 178
normal i 179
normal i 180
normal i 181
normal i 182
normal i 183
normal i 184
normal i 185
normal i 186
normal i 187
normal i 188
normal i 189
normal i 190
normal i 191
normal i 192
normal i 193
normal i 194
normal i 195
normal i 196
normal i 197
normal i 198
normal i 199
normal i 200
normal i 201
normal i 202
normal i 203
normal i 204
normal i 205
normal i 206
normal i 207
normal i 208
normal i 209
normal i 210
normal i 211
normal i 212
normal i 213
normal i 214
normal i 215
normal i 216
normal i 217
normal i 218
normal i 219
normal i 220
normal i 221
normal i 222
normal i 223
normal i 224
normal i 225
normal i 226
normal i 227
normal i 228
normal i 229
normal i 230
normal i 231
normal i 232
normal i 233
normal i 234
normal i 235
normal i 236
normal i 237
normal i 238
normal i 239
normal i 240
normal i 241
normal i 242
normal i 243
normal i 244
normal i 245
normal i 246
normal i 247
normal i 248
normal i 249
normal i 250
normal i 251
normal i 252
normal i 253
normal i 254
normal i 255
normal i 256
normal i 257
normal i 258
normal i 259
normal i 260
normal i 261
normal i 262
normal i 263
normal i 264
normal i 265
normal i 266
normal i 267
normal i 268
normal i 269
normal i 270
normal i 271
normal i 272
normal i 273
normal i 274
normal i 275
normal i 276
normal i 277
normal i 278
normal i 279
normal i 280
normal i 281
normal i 282
normal i 283
normal i 284
normal i 285
normal i 286
normal i 287
normal i 288
normal i 289
normal i 290
normal i 291
normal i 292
normal i 293
normal i 294
normal i 295
normal i 296
normal i 297
normal i 298
normal i 299
normal i 300
normal i 301
normal i 302
normal i 303
normal i 304
normal i 305
normal i 306
normal i 307
normal i 308
normal i 309
normal i 310
normal i 311
normal i 312
normal i 313
normal i 314
normal i 315
normal i 316
normal i 317
normal i 318
normal i 319
normal i 320
normal i 321
normal i 322
normal i 323
normal i 324
normal i 325
normal i 326
normal i 327
normal i 328
normal i 329
normal i 330
normal i 331
normal i 332
normal i 333
normal i 334
normal i 335
normal i 336
normal i 337
normal i 338
normal i 339
normal i 340
normal i 341
normal i 342
normal i 343
normal i 344
normal i 345
normal i 346
normal i 347
normal i 348
normal i 349
normal i 350
normal i 351
normal i 352
normal i 353
normal i 354
normal i 355
normal i 356
normal i 357
normal i 358
normal i 359
normal i 360
normal i 361
normal i 362
normal i 363
normal i 364
normal i 365
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
20
[ 5  2 19 16 11 22 17 24 23 14  1 10 13  8  6 18  4  9  7 20]
seizure i 0
seizure i 1
seizure i 2
seizure i 3
seizure i 4
seizure i 5
seizure i 6
seizure i 7
seizure i 8
seizure i 9
seizure i 10
seizure i 11
seizure i 12
seizure i 13
seizure i 14
seizure i 15
seizure i 16
seizure i 17
seizure i 18
seizure i 19
seizure i 20
seizure i 21
seizure i 22
seizure i 23
seizure i 24
Done reading in 366 no seizure hours and 25 seizure hours
Read in dataset from test_2 ...
Processing data ...
read data and preprocess (fft and slicing)
read in channels
/home/eavsteen/seizure_detection/data/test_2/2_
[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341
 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359
 360 361 362 363 364 365]
292
[106 259  45  26  78 347  90 311 239   6 120 141 263 286 299 210 294 101
 221 140 142 199  59 293 302 254  68 332 113 191 124 102  15  37  54   5
  60 132 264  64 206 303 212 260  20 305 213 214 240  65 173 220 235 164
  92  55  52 135  89  76 326 270   7 100 327 343  56 122 219 159  74 170
 114 295  63 241 306 317 245 329  21 144 146  49 362 341 309 204 269 281
 289  22 208 229 200 136 116  66 134  12 354 158 296 175  17 217 145 318
 234 218 103 356 198 284 268 179 300 150 225  81   1 153   8 266 247 155
 126 283 133 272 255  33 345  34 365  97  85 215 310 162 358 186 168 108
  46 304 181  73 137 348 167 282  29 261 248 298 320  27 231 176 156 171
 138 190 276 246 252  35 230 216 233 236  44 357 129  93 111 166 250 224
  18 258  79 322  71 228 307 352 196 160 360 161  83 189 363 118 331 237
 342 110 149 157 152  16 238  75 109 297 205 253 139 359   4  96  14  61
  67 316 194 154 223 280 355 340  40 112  13 107 226 278   3 249 125  24
  30  77 232 188 184 338 274 346  19 312 350 182 324  80  51   2  11 104
 271  86  10  36 349  58  41  50 209 328 287 301 123 222  62 275 130 187
  23  43   0 201 308  98 178 256  94 336  95 319 169  69  48 207 279 227
 148 143 180 131]
normal i 0
